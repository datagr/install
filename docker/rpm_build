#!/usr/bin/env python
import sys
import os
import shutil
from contextlib import contextmanager

import build
import build.util
import build.docker


# The purpose of this script is to generate a tar file distribution of
# opm/flow. The script is based on docker, and the functionality can be briefly
# summarized as follows:
#
#  1. Start with a plain docker image for e.g. Centos or Ubuntu, and then build
#     a new image by running the equivalent of "apt-get install opm".
#
#  2. On the newly created image we run the command "docker save" which will
#     extract the layers in the docker image, from this we can identify the
#     layer which actually installs flow, and then get a listing of all the
#     files in that layer - these are the files we are interested in.
#
#  3. To actually extract the interesting files from the docker image we need
#     to start a docker container which mounts an outside path and then run a
#     script inside the container which copies all the interesting files to the
#     externally visible path.
#
#     Observe that the script also creates a docker image which can be
#     installed, that is not actually used.
#
#  4. Finally the script creates a small README and a small enable script,
#     copies in two folders of testdata and packs everything in .tar.gz file.
#
# The behaviour of the script is goverened by a small config file, which is
# given as commandline argument. This config file should contain the path to
# dockerfiles and template files for the enable script and readme files.
# Currently the configurations are organized with one complete configuration in
# a directory by itself.




def make_enable(config, flow_prefix):
    template = open(config.enable_in).read()
    with open("{}/enable".format(flow_prefix), "w") as f:
        f.write(template.format(flow_prefix = flow_prefix))


def make_README(config, flow_prefix):
    template = open(config.readme_in).read()
    with open("{}/README".format(flow_prefix), "w") as f:
        f.write(template.format(version = config.version,
                                flow_prefix = flow_prefix))


def copy_data(src_path, target_path):
    shutil.copytree(src_path, target_path)


def print_msg(tar_file):
    msg = """
Push tar file to google storage:

    gsutil cp {0} gs://datagr-export

Make file public:

    gsutil acl ch -u AllUsers:R gs://datagr-export/{1}

URL:

    https://storage.googleapis.com/datagr-export/{1}
""".format(tar_file, os.path.basename(tar_file))
    print(msg)



script_path = os.path.abspath( os.path.dirname(__file__) )
config = build.Config(sys.argv[1])
if config.mpi:
    flow_version = "2019.04-mpi"
else:
    flow_version = "2019.04"

data_path = os.path.join(script_path, "testdata")

# Create the main docker image where opm/flow is installed.
base_id = build.docker.build(config.base_file)

with build.util.tmpd():
    # Make a list of all the files with prefix /usr from the layer where
    # opm/flow is installed
    files = build.docker.installed_files(base_id, "opm-simulators-", build.docker.prefix_usr)
    with build.util.tmpd():
        flow_prefix = "flow-{}".format(flow_version)
        # Run a docker container with a script to pull out all the files in the
        # list created by installed_files().
        build.docker.extract_files(base_id, files, os.path.join( os.getcwd(), flow_prefix))

        make_enable(config, flow_prefix)
        make_README(config, flow_prefix)
        copy_data(data_path, "{}/testdata".format(flow_prefix))
        tar_file = build.util.make_tarfile(flow_prefix)
        print("Tar file: {} created".format(tar_file))
        with build.util.pushd(flow_prefix):
            build.docker.build(config.install_file, "{}-{}".format(config.os_image, flow_version))

print_msg(tar_file)

